{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271IL-VYk0KT",
        "outputId": "a7744b9c-d234-4659-e3b4-c181fbcf8ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/10, Loss: 7523462.2106\n",
            "Epoch 2/10, Loss: 7507331.7654\n",
            "Epoch 3/10, Loss: 7360076.2931\n",
            "Epoch 4/10, Loss: 7278070.7343\n",
            "Epoch 5/10, Loss: 7221587.0452\n",
            "Epoch 6/10, Loss: 7178375.7396\n",
            "Epoch 7/10, Loss: 7144252.8949\n",
            "Epoch 8/10, Loss: 7114454.1199\n",
            "Epoch 9/10, Loss: 7089366.5954\n",
            "Epoch 10/10, Loss: 7067221.5159\n",
            "Word embeddings saved to 'word_embeddings.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Sigmoid function with clipping to prevent overflow\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -15, 15)))\n",
        "\n",
        "# Data Preprocessing\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text.lower()).split()  # Remove punctuation and lowercase\n",
        "\n",
        "# Building vocabulary\n",
        "def build_vocab(corpus):\n",
        "    return Counter(word for sentence in corpus for word in sentence)\n",
        "\n",
        "# Generating skip-grams\n",
        "def generate_skip_grams(sentence, window_size=2):\n",
        "    return [\n",
        "        (sentence[idx], sentence[context_idx])\n",
        "        for idx in range(len(sentence))\n",
        "        for context_idx in range(max(0, idx - window_size), min(len(sentence), idx + window_size + 1))\n",
        "        if idx != context_idx\n",
        "    ]\n",
        "\n",
        "# Training Word2Vec embeddings\n",
        "def train_word2vec(skip_grams, vocab_size, word_to_idx, embedding_dim=10, lr=0.01, epochs=10, negative_samples=5, batch_size=512):\n",
        "    W1 = np.random.uniform(-1, 1, (vocab_size, embedding_dim)) / np.sqrt(vocab_size)\n",
        "    W2 = np.random.uniform(-1, 1, (embedding_dim, vocab_size)) / np.sqrt(embedding_dim)\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(skip_grams)\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(0, len(skip_grams), batch_size):\n",
        "            batch = skip_grams[i:i + batch_size]\n",
        "            batch_center = np.array([word_to_idx[center] for center, _ in batch])\n",
        "            batch_target = np.array([word_to_idx[target] for _, target in batch])\n",
        "\n",
        "            h = W1[batch_center]  # Hidden layer\n",
        "            u = np.dot(h, W2)  # Output layer\n",
        "            y_pred = sigmoid(u)\n",
        "\n",
        "            # Generate negative samples\n",
        "            labels = np.zeros((len(batch), vocab_size))\n",
        "            labels[np.arange(len(batch)), batch_target] = 1\n",
        "\n",
        "            # Backward pass\n",
        "            error = y_pred - labels\n",
        "            grad_w2 = np.dot(h.T, error)\n",
        "            grad_w1 = np.dot(error, W2.T)\n",
        "\n",
        "            # Loss\n",
        "            total_loss -= np.sum(np.log(y_pred[np.arange(len(batch)), batch_target] + epsilon))\n",
        "\n",
        "            # Update weights\n",
        "            W2 -= lr * np.clip(grad_w2, -5, 5)\n",
        "            W1[batch_center] -= lr * np.clip(grad_w1, -5, 5)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return W1, W2\n",
        "\n",
        "# Main Code\n",
        "if __name__ == \"__main__\":\n",
        "    data = pd.read_csv('/content/drive/MyDrive/spam_or_not_spam.csv').dropna()\n",
        "    data['clean_text'] = data['email'].apply(clean_text)\n",
        "\n",
        "    # Balancing the dataset\n",
        "    spam = data[data['label'] == 1]\n",
        "    not_spam = data[data['label'] == 0].sample(len(spam), random_state=42)\n",
        "    balanced_data = pd.concat([spam, not_spam])\n",
        "\n",
        "    # Building vocabulary and generating skip-grams\n",
        "    vocab = build_vocab(balanced_data['clean_text'])\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab.keys())}\n",
        "\n",
        "    training_pairs = [pair for sentence in balanced_data['clean_text'] for pair in generate_skip_grams(sentence)]\n",
        "\n",
        "    # Training Word2Vec\n",
        "    vocab_size = len(vocab)\n",
        "    W1, W2 = train_word2vec(training_pairs, vocab_size, word_to_idx)\n",
        "    word_embeddings = {word: W1[word_to_idx[word]] for word in word_to_idx.keys()}\n",
        "\n",
        "    # Saving embeddings to a CSV file\n",
        "    embeddings_df = pd.DataFrame.from_dict(word_embeddings, orient='index')\n",
        "    embeddings_df.to_csv('/content/drive/MyDrive/word_embeddings.csv', header=False, index=True)\n",
        "    print(\"Word embeddings saved to 'word_embeddings.csv'.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}